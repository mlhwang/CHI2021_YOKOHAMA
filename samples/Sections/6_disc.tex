\vspace{-5pt}
\section{Discussion}



% What are the questions we asked
We piloted Monster Munch (N=68) as a feasibility-focused study to test out two main mechanisms: player avatar relationships and a crowdsourced community board as a resource for decision making in the app. Though avatars are frequently utilized in gamified apps and games, there are few instances of them embodying learning content and paired with crowdsourced social features. Beyond understanding the impact on enjoyment of the app that users experienced with these mechanisms, we wanted to understand the level to which these mechanisms encouraged users to engage with nutrition topics -- a notoriously difficult task in the space of nutritional education.



% Maybe third contribution becomes first so that we can mention this subsection first. IDK --- (3) This study emphasizes the role that player-avatar-identification may have in shaping users' engagement and enjoyment of the app and possible effects on recollection and learning outcomes. \textcolor{red}{OR is this contribution really just another way of saying (1) and (2)???}



% key contributions:    (notes based on new edits to intro)

% 1 using gamification and social mechanisms to help deliver nutritional info in a new way
%    the integration of the two mechanisms (avatars and CB)

% 2 emphasizes the role of PAID in influencing engagement/enjoyment
%    the role of PAID related to engagement

% 3 novel integration ... may have implications for nutritional literacy and recollection
%    the potential for nutritional learning (because of #1)


%%%%%% BOTH KINDS OF MECHANISMS %%%%% CONTRIBUTION 1 %%%%%%
\vspace{-5pt}
\subsection{Gamification and Social Mechanisms for Nutritional Information Delivery}



% concisely highlight key findings here - what do these findings mean, what are you about to argue below?
We found that our users enjoyed using the app and their self-reported confidence for assessing macronutrient content in meal photographs increased post-app usage.
A key insight was that specifically, identification with the pet monster avatar, as well as engagement with the community board, was highly predictive of enjoyment, rating of the app, and recommending the app to others. 
Learning was only moderately shown with those who did not identify with their avatars highly, showing that further investigation is due for measuring learning outcomes as a by-product of exploring gamification mechanisms. 



%%%%%%%% ENGAGEMENT/ENJOYMENT %%%%% CONTRIBUTION 2 %%%%%%
\vspace{-5pt}
\subsection{Gamification Mechanisms for Positive User Experience and Engagement}


Overall, we found users reported positive experiences with their pet monster avatars. The Player-Avatar-Identification (PAID) score was a strong predictor of enjoyment, rating, and recommendation of the app to others. 
%In the post-task, we asked users to rate how much they liked using monsters to learn about meals and nutrition and majority of the users expressed \textcolor{red}{report STATS} liking the monster avatars a great deal as a vehicle for delivering meal correctness feedback. -- we didn't ask this in PP
In particular, participants who reported having a high school education or less identified significantly more with their avatars, possibly because it is generally more accessible or easier to retrieve nutritional information through visual effects as opposed to text. In addition, we purposefully chose monster avatars with the intention of eliminating additional complexity that can be derived from various aspects such as gender influence. This aspect probably aided in terms of receiving majority positive feedback regarding the pet monster avatars.  

In the pilot study, users were given the opportunity to choose a monster avatar with specific health goals and additionally asked to name their pet avatar. 
Users commonly reported selecting an avatar because it had the same nutritional goal as themselves or someone they knew, and because they liked the avatar's appearance. 
It is possible that these features helped the users feel more associated with the characters by taking a form of ``ownership'' of the avatar. 
Technical abilities~\cite{newman2002myth} and empathy capacity~\cite{cohen2006audience} of the character, as well as a visually appealing avatar~\cite{kim2012became}, have been recognized as factors that assist with player-avatar identification~\cite{turkay2014effects}.
Users ``controlled'' how the avatars would look by selecting the correct or incorrect meal to feed them and received instantaneous feedback based on how their avatar responded.
Additionally, some users were able to ``unlock accessories'' for their avatars and customize their appearances (see Figure~\ref{fig:monster-stages}) which may have assisted with connecting with their avatar, as cosmetic avatar customization can be strongly related to identification~\cite{turkay2014effects}. 
Though the interaction time with the pet monster avatar in the app was not very long ($\sim$217sec), many users unprompted noted finding the monster avatars ``cute'' and ``adorable'' in the open feedback question about their study experience, which supports the positive evaluation towards the avatars.

%Unfortunately, most users did not correctly complete enough rounds (at least four consecutive rounds) to ``unlock accessories'' for their avatar. 

%however, most users did not know about the accessories. 
%having more options for avatar customization that users were aware of should be considered in the future. 


%%%%%%%%%%%%% LEARNING %%%%%%%%% CONTRIBUTION 3 %%%%%%
\vspace{-5pt}
\subsection{Gamification and Social Mechanisms on Nutritional Learning}
%users who indicated greater use of the community board performed significantly worse on cursory measures for recall after controlling for prior nutritional knowledge 
We were surprised to find that (1) lower PAID scores marginally predicted greater recall of nutrition information at post-test, even after controlling for baseline nutrition knowledge, educational background, and Monster Munch app performance; and (2) users who indicated using greater use of the community board (CB) performed worse for recall measures after controlling for prior nutrition knowledge.  
Use of the CB might have made people more reliant on the community and resulted in them taking less ownership of their own learning and resulting in lower recall scores. Additionally,
it is possible that the evaluation of the ``in-the-wild'' meal photos and crowdsourced-based CB information were overwhelming and a lot to process for our users. 
One study~\cite{makransky2019adding} showed that placing university students in an immersive virtual science world through VR headsets led them to higher presence in the virtual learning simulation, but resulted in poorer learning outcomes and higher cognitive loads. 
Our users had to process quite a bit of information in each round in the app: from reviewing meal options with their component descriptions to inputting explanations, all the while having to digest the crowd's evaluations as well. 
%and process that information; (2) write a text-based reasoning for their initial choice to feed the pet avatar; (3) process the information in the CB that included voting and reasoning for all four meal options by the community; and (4) provide an additional text-based reasoning for their final choice to feed their monsters.
%Only then they were given correctness feedback of the meal they decided to feed their monster avatar. This was repeated five times in total. 
Also, the CB results were taken from the Data Collection Phase and therefore did not always indicate the correct meal as the most voted meal as the ``correct meal.'' If users wholeheartedly trusted the community's opinion but then immediately found out that their original meal choice was correct, there could be possible confusion and trust issues with the crowd that could develop. 
In addition, while the app always gave four meal options to choose from, the pre- and post-assessment questions were always a choice between two meal options and correctness feedback was never provided, which complicates the definition of recall learning. 
Further investigation is necessary to measure specific learning outcomes that mimic the intervention activities more closely to see if avatar identification emerges as an important possible predictor of nutritional knowledge and literacy.

% It is possible that the correlation we are seeing between higher PAID scores and lower learning scores suggests that the users that related more to their avatars with nutritional goals because they themselves struggle with nutrition? 

% This would suggest that the user would have higher engagement with an extended version of the Monster Munch app, leading to a future increased nutritional learning?

Though not directly related to learning outcomes per se, our study showed that users' self-reported confidence in their ability to assess macronutrients increased after using the app. This shows great potential for photo-based macronutrient nutrition education, as confidence in learning is shown to be correlated to actual learning outcomes ~\cite{badura1995exercise,choi2005self,de2013relationship,nicholson2013key,wesson2011self}. One user mentioned that this app has great potential as an education app (ID-685) and another user said: \textit{``If people care about their nutritional intake, it is a cute [app]! I am a vegetarian so not the biggest fan about seeing many pictures of meat.''} (ID-278).
This shows that there is potential to improve the current app to cater meal photos to individual's dietary preferences and restrictions and this could further aid in the app becoming a more effective educational teaching tool. 
While this study does not directly measure self-efficacy, future work should examine how user's self efficacy changes through interaction with the app. 
% -- in general, though it was not required, majority wrote a positive response about the study (most were short like , fun, good, awesome, but all still positive and question was not required to input anything)
% \textit{``it seems interesting, best of luck! I would definitely use this app.''} (ID-043845)
% Importantly, these results suggest the affinity between players and their avatars may be an important component to the overall positive experience with the app.

Similarly we saw that user's self-reported frequency of using the CB's suggestion (to determine what to feed their monster avatar) significantly predicted their enjoyment, rating, and recommendation of the app. 
Together these findings highlight that user's engagement with gamification and social mechanisms are important components to their overall experience using and enjoying the app. They also suggest more thorough research focused on learning outcomes is needed to understand exactly how these game mechanisms impact users' learning.  

% Theory of behavior change --- engagement is one of the first steps of this theory
% learning takes a long time so they behavioral change , so we are looking for more active learning , how do you attain sustainment, or engagement, continue users to inquire about nutrition topics, well how do you get that? with ENGAGEMENT



\vspace{-5pt}
\subsection{Limitations and Future Work}


% --- even though the negative comment about the app being for kids and silly is from DCP, this may be an explanation why there was a low PAID score with higher educated people. Maybe the perception was that, the monsters were less relateble because it seemed a bit childish for them. 

% \subsubsection{Limitations with the Gamification Mechanisms}

There are a few limitations to consider in this study that can inform future work. First, it is important to note that Monster Munch was designed as a proof-of-concept pilot study designed to test a limited set of gamification mechanisms. While this version of the application was not specifically designed to evaluate how game mechanisms drive user learning, we hope these insights can inform future work about key mechanisms to focus on to build useful and gameful education tools. Further, the effects reported within this paper were observed after users had interacted with the app only one time, while most nutritional learning apps are intended to be used multiple times for sustained learning. It is possible that the role of gamification features and effect on confidence and learning may differ after using the app several times.

Second, while this app utilized avatars that changed states in response to app performance to reinforce healthy nutrition choices, players were not made aware of the full range of forms their avatar could take. Only users who answered four consecutive rounds (out of a total of five) correctly ``unlocked accessories'' for their avatar. It is possible that had users known how they performed in the context of possible performance states, they would be more motivated to answer correctly and spend time reflecting on nutrition choices and within the app activities. 

Additionally, Monster Munch was not a full fledged game but an app with gamification features. Therefore, it lacked some key game mechanics such as having a final ``winning'' or ``losing'' stage for the players. Adding a clear ``winning state'' (e.g., advancing to a new level or achieving the avatar's health goal) may have facilitated even greater user investment in the app and fostered user's motivation to continue playing for improvement. 
Future studies can also examine this motivation through the addition of a ``play again'' button and allowing users to select the same or a different monster avatar. 
These strategies would be useful ways to measure engagement and enjoyment in the next phases of this project.  

Finally, in this initial pilot, meals presented throughout the Monster Munch app were closely reviewed by nutrition experts to create a clear gold-standard for the macronutrient content of meals. 
However, this limits the range of meals presented and requires considerable work from the study team. In future iterations of this study, we will use the data collected in our pilot and include the voting and reasoning for meal choices in the community board. In this way the community board will be created, maintained, and updated truly by the crowd's input and opinion.  

In future work, we hope to develop Monster Munch into a fully-fledged game that has winning/losing states, uses a more robust crowdsourced community, contains more meal photographs with their evaluations and reasons, and includes competitive game mechanics so that players can play against each other.  
%so that they have to achieve their health goal faster in a given amount of time. 
This will add the competitive nature that is popular in many successful games. 


% Given the easy nature of data collection, Amazon Mechanical Turkers (AMTs) were used for the data collection phase and half of the pilot participants were AMTs. We did receive quite a variety of responses with the AMTs, and in particular, when they were asked to provide a text-based reason for their meal choices before the community board (CB) and after CB, it became clear that many were not reading the instructions or did not put much effort into the task. We rejected a number of HITs as those responses were of ``low-quality'' from our research perspective. Whether we rejected and approved the ``right'' responses will be difficult to assess in any capacity. 

% How much can we guarantee that the post-test responses were not influenced by fatigue. (Maybe this is a good place to quickly mention the average duration people took to complete the entire study? -- first mention this in the Evaluation section when maybe talking about Participants)


% \subsubsection{Determining the Gold Standard for ``in-the-wild'' Meal Photographs}
% First of all, we involved quite a number of people to achieve this. But also the meal photographs are from our previous studies~\cite{} -- keep it anonymized-- and that means most meals are from a particular geographical population (Washington Heights). In creating a true ``crowdsourced intelligence'' driven community board we would have to do a much more comprehensive collection of meal photographs. Maybe using soical media sites like IG and TikTok might help?

% Taken from \cite{cuthbert2019effects}:
% ``Secondly, this study had a small number of participants
% which made the statistics very vulnerable to type II errors.
% We tried to deal with this limitation by reporting effect sizes,
% descriptive statistics, and all the p values, avoiding the typical
% practice of sharing only statistically significant results.''


%(\textcolor{orange}{why not use the users' evaluations instead? What is our rationale for this? if we wanted a crowdsourced CB in the truest sense, we should have taken the users' assessments. so why?}

% \textcolor{red}{How do you guys think we should justify and include this?????}
% \textcolor{orange}{Maybe we can focus less on the meals that had zero percent, and instead frame it as filling gaps for meals that had a limited number of viable reasons (below three)?}

% For meal options that were clearly not preferred and fitting the nutritional goal of the monster avatar, we had no text-based reasoning/descriptions for why that particular meal would be the ``right'' choice. Therefore, we had to request our users in the Data Collection Phase to assume that those meals were the right meals for the nutritional goal and provide text-based reasoning for why the meals fit a particular nutritional goal. Moving forward meals that completely do not fit a nutritional goal, and therefore, were not voted for by the crowd should be left empty with no text-based reasoning. If it is clearly not the right choice by the crowd, then let that evaluation show in the community board.  
%(\textcolor{orange}{Again, why not use the users' true evaluations instead? As in there were ZERO reasons for a meal to fit a nutritional goal. What is our rationale?}

%the true form of what we want to produce is far from what we tested here. In fact, though we would love a full-fledged game (we did not quite make it a game and only tested out a few gamification mechanisms) freely used by the crowd and the content driven by the crowd, in this particular pilot, we had to pre-process a lot of the data ourselves. The meal photographs were from real users of an app developed in our previous work (keep anonymous) but the gold standard for their evaluation were taken from professional dietitians as we needed a starting point. In future iterations of this study, we will use the data collected in our pilot and include the voting and reasoning for meal choices in the community board. In this way the community board will be created, maintained, and updated truly by the crowd's input and opinion.  

% How do we get to a truly crowdsourced community board that really represents all people and all types of food? Is that even possible? Could there be one place, one repository for this, or should there not be just one repository? How can we do this more at scale?

%\textcolor{orange}{This app with gamification and social mechanisms is a platform that has a potential to grow and aggregate many more crowdsourced, ``in-the-wild'' meal photographs that are also evaluated by the crowd. Given that we involved so many experts, and the gold standard for the meals used were also from trained dietitians, and it took us considerable amount of time to filter through the meals, this whole lightweight and ``naturally crowdsourced'' claim may be difficult. So how do we avoid that attack from the reviewers?????????}


