\vspace{-5pt}
\section{Monster Munch Pilot Evaluation Study}

The pilot evaluation study for the Monster Munch app was conducted with participants recruited via 
Amazon Mechanical Turk (AMT) and social media sites. The goals of the evaluation study were to (1) to assess users general perceptions using a lightweight app for engaging with nutrition and (2) understand how the different gamification mechanisms built into the app (avatars and crowdsourced community board) shape users preferences and experience with the app. The pilot study followed the procedure outlines below and summarized in Figure~\ref{fig:studyflow}. 


\begin{figure}[h]
\includegraphics[width=\textwidth]{samples/images/figure-1.png}
\caption{Study flow for participants in the Monster Munch pilot study. Study sections indicated inside the curly braces were completed within the mobile app. }
\label{fig:studyflow}
\end{figure}

\vspace{-5pt}
\subsection{Participant Recruitment}
Participants were recruited from AMT and social media sites (Facebook and Instagram). Users were screened with the following inclusion criteria: 1) own an Android mobile device, 2) above 18 years of age. After accepting the terms of the study found in the consent form, participants  downloaded an APK file and installed the Monster Munch app. Within the app they completed a pre-test evaluation, completed the nutrition task with monsters, and completed a post-test assessment and surveys. Only participants who completed the pre-test evaluation, the Monster Munch app, and post-test evaluation were considered in subsequent analysis. 


\vspace{-5pt}
\subsection{Monster Munch App Activities}

In the Monster Munch app, users could view each of the four pet monster avatars and their nutritional goals before selecting one to proceed with and ``help'' during the game. Users also had the opportunity to give their monster a custom name before completing the task and helping feed their monster.

In the core monster feeding task, users selected meals to feed their chosen monster for five rounds. Each round of meal selection consisted of five steps. 
At the first stage of each round, the user was presented with  four options of ``in-the-wild'' meal photographs, accompanied with brief descriptions of the contents of each meal that they could review to decide what to feed their monster. Second, users were asked to select the meal that they believed best fit their monster’s nutritional goal and provide a short text-based description of why they selected that particular option. Third, users viewed the crowdsourced CB where they could consider which meal other members of the community chose to feed their monster. Users could view the percent of users that selected each meal and their reasoning for doing so (these percents and rationales were sourced from the implementation pilot described above). Fourth, armed with this new information, users had the option to keep their original meal selection, or switch to another meal option. Again, users were asked to provide a short description to rationalize their final meal selection after viewing the input from the CB.
Finally, after they submitted their reasoning, the app told the user whether the meal option they selected was (in)correct, and which meal option was the best choice. The appearance of the monster avatar also changed in response to the user's performance, becoming ``healthier'' if the user selected the correct meal, and ``less healthy'' if the user selected the incorrect meal. 

Users repeated these steps for each five meal selection rounds with their same  monster avatar, helping them work towards their monster's nutritional goal.
\vspace{-5pt}
\subsection{Data Analysis and Measures of Interest}
Participants' survey responses were collected via Qualtrics and their engagement with the app  collected via the Google Cloud computing platform Firebase and extracted with Python 2.7 (script available on Github at \url{provide_later}). Users' data from Qualtrics and Firebase were merged together for processing, and 
%\url{https://github.com/mlhwang/m4m}
Statistical Package for the Social Sciences (SPSS version 27) was used to analyze the data. For all analysis the alpha significance threshold was set at 0.05.  

\subsubsection{Player Avatar Identification}
Users' engagement with the gamified avatar component of the task was assessed with four Player Avatar Identification (PAID) questions asked in the post-task survey (adapted from~\cite{li2013player}). Users indicated their agreement to the following four statements on a 7-point likert scale (with the option to select ``Not Applicable''): 
\begin{enumerate}
    \item ``When my monster’s condition worsened, I felt angry/sad.''
    \item ``When my monster achieved their goals, I felt happy.''
    \item ``My monster reflects who I am.''
    \item ``My monster influences the way I feel about myself.'' 
\end{enumerate}
A ``PAID score'' was calculated for each user by summing their responses to each question. Higher PAID scores indexed stronger player-avatar identification.

\subsubsection{Community Board Agreement}

The community board (CB), was created through the Data Collection Phase. The two CB related questions during the post-app task included: 
\begin{enumerate}
    \item ``Did the community board influence your final meal choice?'' 
    \item ``How often did you agree with the rest of the community?'
\end{enumerate}

Users responded on a scale of `Always;' `More than half of the time;' `About half of the time;' `Less than half of the time;' `Never;' and `I did not look at the community board.' 

\subsubsection{Macronutrient Knowledge Assessment}

Macronutrient knowledge was evaluated before and after the Monster Munch app activities. Users completed a 12-question pre-test for which a percentage correct was calculated to establish a baseline for nutritional knowledge. After the Monster Munch app, users completed an 18-question post-test, that contained the 12-questions asked in the baseline evaluation and six additional questions. Performance on the 12 repeated questions was used to assess recall of nutrition knowledge, and performance on the six new questions was used to assess transfer of nutrition knowledge in comparison to the baseline evaluation. 
\vspace{-5pt}
\subsection{Research Questions}
In the next section we discuss the results from this pilot study using Monster Munch. In particular this pilot evaluation was centered around understanding the following questions:
\begin{enumerate}
    \item Does playing Monster Munch influence users' confidence of their ability to estimate macronutrient content of ``in-the-wild'' meal photographs (a proxy for nutritional engagement)?
    \item Does the inclusion of gamification mechanisms (pet monster avatars and crowdsourced community board) influence enjoyment/engagement of the Monster Munch app?
    \item Do gamification mechanisms have an effect on nutritional learning (a proxy for nutritional engagement)?
\end{enumerate}

% The hypotheses are:
% \begin{itemize}
%     \item \textbf{H1}: User's will report higher enjoyment and show greater engagement with the Monster Munch app after the inclusion of Gamification Mechanisms. 
%     \item \textbf{H2}: Monster Munch (a mobile app for nutritional engagement) will help users become more confident in their ability to assess macronutrient content of ``in-the-wild'' meal photographs pre- to post-test.
%     \item \textbf{H3}: Users who chose pet avatars with a health goal that is personally meaningful in Monster Munch are more likely to perform better (i.e., identify the right meal photographs for a specific nutritional goal) pre- to post-test.

% \end{itemize}

%\subsubsection{Data Collection Phase (DCP)} The three primary goals of the DCP were to 1) collect user-generated responses to use within the `Monster Munch' community board in the EP, 2) provide initial insights on the prior knowledge and nutritional literacy of lay users, and 3) assess the user experience and data collection methods of the app. To accomplish these goals, users in the DCP completed both a pre- and post-task, and a simplified version of the app, without the gamification mechanisms (GM) used in the EP. In the DCP, every user was assigned one of the four pet monster avatars. The user was introduced to their monster avatar and its nutritional goal before proceeding to the five rounds. 

%There will be three steps for the user in each of the five rounds. First, the user was presented with the four options of ``in-the-wild'' meal choices, where they selected the meal that they believed best fit their monster's nutritional goal. Next, the user was asked to provide a short description of why they selected that particular option. Finally, after the user submitted their reasoning, the app revealed whether the meal option they selected was correct, and if they were incorrect, which meal option was the best choice. Each user repeated these three steps for all five rounds with the same pet monster avatar and its nutritional goal. 

%The app in the EP differs from the DCP with the addition of the GMs.

%%% Is this sort of thing needed for CHI?%%%
%\subsection{Analysis}
%We will use a combination of qualitative and quantitative methods to analyze the data collected during the proposed study. For the quantitative measures collected during the study (results of their scores on different questionnaires and accuracy of their nutritional assessment), we will use appropriate statistical tests to examine the differences in means for the measures of interest.